{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9d657f-6bce-4568-ba13-fafbcaf8d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4a58d-2ac5-487b-b18d-79aca638165c",
   "metadata": {},
   "source": [
    "**Question1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6744ecf-5f18-4f71-be46-7958d8595473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56255f38-36fa-488d-ac1a-530814e8d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['This', 'is', 'the', 'first', 'assignment', 'of', 'natural', 'language', 'processing', '.']\n",
      "Number of words: 10\n",
      "Word frequency: Counter({'This': 1, 'is': 1, 'the': 1, 'first': 1, 'assignment': 1, 'of': 1, 'natural': 1, 'language': 1, 'processing': 1, '.': 1})\n",
      "POS tags: [('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('assignment', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "text = \"This is the first assignment of natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Count the number of words\n",
    "word_count = len(tokens)\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_frequency = Counter(tokens)\n",
    "\n",
    "# Identify words belonging to different POS (Part of Speech) tags\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Tokenized text:\", tokens)\n",
    "print(\"Number of words:\", word_count)\n",
    "print(\"Word frequency:\", word_frequency)\n",
    "print(\"POS tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758254cb-f594-49e4-9998-689ee177bd33",
   "metadata": {},
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da27176-5eb9-4bd5-a9cd-3c56ddb69b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'natural' is an English word.\n",
      "'Punjab' is not an English word.\n",
      "'nonexistent' is an English word.\n",
      "'programming' is not an English word.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "\n",
    "# Check if a word is in the English words corpus\n",
    "def is_english_word(word):\n",
    "    from nltk.corpus import words\n",
    "    return word.lower() in words.words()\n",
    "\n",
    "# Input string(s) to check\n",
    "strings_to_check = [\"natural\", \"Punjab\", \"nonexistent\", \"programming\"]\n",
    "\n",
    "for string in strings_to_check:\n",
    "    if is_english_word(string):\n",
    "        print(f\"'{string}' is an English word.\")\n",
    "    else:\n",
    "        print(f\"'{string}' is not an English word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06385d62-d5e0-423a-8c13-8d6652d6c6aa",
   "metadata": {},
   "source": [
    "**Question3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83c2317-906b-4279-9dbb-1d7ffabdc9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum edit distance between 'sweet' and 'swift' is 4.\n"
     ]
    }
   ],
   "source": [
    "def min_edit_distance(str1, str2):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "\n",
    "    # Create a table to store the edit distances\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    # Initialize the first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Calculate the edit distance\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 2\n",
    "\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,     # Deletion\n",
    "                dp[i][j - 1] + 1,     # Insertion\n",
    "                dp[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    # The minimum edit distance is in the bottom-right cell\n",
    "    return dp[m][n]\n",
    "\n",
    "# Example usage\n",
    "str1 = \"sweet\"\n",
    "str2 = \"swift\"\n",
    "distance = min_edit_distance(str1, str2)\n",
    "print(f\"Minimum edit distance between '{str1}' and '{str2}' is {distance}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c1bec-e057-4642-9288-99b4bff1157a",
   "metadata": {},
   "source": [
    "**Question 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57603936-daf2-4a64-aae2-d6389e14958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: running\n",
      "Morphological form: run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create a Porter Stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Input word to find morphological form\n",
    "input_word = \"running\"\n",
    "\n",
    "# Get the stem of the word\n",
    "morphological_form = stemmer.stem(input_word)\n",
    "\n",
    "print(f\"Input word: {input_word}\")\n",
    "print(f\"Morphological form: {morphological_form}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa1612-7701-467d-a85d-30d7c9695dd6",
   "metadata": {},
   "source": [
    "**Question 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fa31e8-5c1a-4f7b-b0ec-5f4d7cdd8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8be017-f8a3-42d5-ae53-e009a7aa35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'quick', 'brown', 'dog', 'runs']\n",
      "Predicted POS tags: ['Noun', 'Verb', 'Adjective', 'Noun', 'Verb']\n",
      "Viterbi Probabilities:\n",
      "Position 1: [0.12 0.   0.  ]\n",
      "Position 2: [0.     0.0072 0.0072]\n",
      "Position 3: [0.       0.       0.000864]\n",
      "Position 4: [3.456e-05 0.000e+00 0.000e+00]\n",
      "Position 5: [0.0000e+00 8.2944e-06 0.0000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define states (POS tags)\n",
    "states = [\"Noun\", \"Verb\", \"Adjective\"]\n",
    "\n",
    "# Define the initial probabilities (adjusted for the example)\n",
    "initial_probabilities = {\"Noun\": 0.4, \"Verb\": 0.3, \"Adjective\": 0.3}\n",
    "\n",
    "# Define transition probabilities (adjusted for the example)\n",
    "transition_probabilities = {\n",
    "    \"Noun\": {\"Noun\": 0.1, \"Verb\": 0.6, \"Adjective\": 0.3},\n",
    "    \"Verb\": {\"Noun\": 0.4, \"Verb\": 0.3, \"Adjective\": 0.3},\n",
    "    \"Adjective\": {\"Noun\": 0.2, \"Verb\": 0.5, \"Adjective\": 0.3},\n",
    "}\n",
    "\n",
    "# Define emission probabilities (adjusted for the example)\n",
    "emission_probabilities = {\n",
    "    \"Noun\": {\"The\": 0.3, \"dog\": 0.2, \"cat\": 0.2, \"ball\": 0.3},\n",
    "    \"Verb\": {\"quick\": 0.1, \"runs\": 0.4, \"jumps\": 0.3, \"barks\": 0.2},\n",
    "    \"Adjective\": {\"brown\": 0.4, \"lazy\": 0.4, \"quick\": 0.2},\n",
    "}\n",
    "\n",
    "# Input sentence\n",
    "sentence = [\"The\", \"quick\", \"brown\", \"dog\", \"runs\"]\n",
    "\n",
    "# Initialize the Viterbi matrix\n",
    "viterbi = np.zeros((len(states),len(sentence)) )\n",
    "backpointer = np.zeros((len(states), len(sentence)), dtype=int)\n",
    "\n",
    "# Initialize the first column of the Viterbi matrix\n",
    "for s, state in enumerate(states):\n",
    "    viterbi[s][0] = initial_probabilities[state] * emission_probabilities[state].get(sentence[0], 0)\n",
    "    backpointer[s][0] = 0\n",
    "\n",
    "# Fill in the rest of the Viterbi matrix\n",
    "for t in range(1, len(sentence)):\n",
    "    for s, state in enumerate(states):\n",
    "        max_prob = 0\n",
    "        max_ptr = 0\n",
    "        for s_prev, prev_state in enumerate(states):\n",
    "            prob = viterbi[s_prev][t - 1] * transition_probabilities[prev_state][state] * emission_probabilities[state].get(\n",
    "                sentence[t], 0)\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                max_ptr = s_prev\n",
    "            viterbi[s][t] = max_prob  # Store the max probability in the Viterbi matrix\n",
    "            backpointer[s][t] = max_ptr  # Store the backpointer\n",
    "\n",
    "# Find the best path by tracing back from the last word\n",
    "best_path = []\n",
    "max_final_prob = 0\n",
    "final_state = 0\n",
    "for s, state in enumerate(states):\n",
    "    if viterbi[s][len(sentence) - 1] > max_final_prob:\n",
    "        max_final_prob = viterbi[s][len(sentence) - 1]\n",
    "        final_state = s\n",
    "\n",
    "best_path.append(final_state)\n",
    "for t in range(len(sentence) - 1, 0, -1):\n",
    "    best_path.append(backpointer[best_path[-1]][t])\n",
    "\n",
    "# Reverse the best path to get the correct order\n",
    "best_path = best_path[::-1]\n",
    "\n",
    "# Map the state indices to actual POS tags\n",
    "best_tags = [states[state] for state in best_path]\n",
    "\n",
    "# Print the result\n",
    "print(\"Input sentence:\", sentence)\n",
    "print(\"Predicted POS tags:\", best_tags)\n",
    "\n",
    "# Print the probabilities\n",
    "print(\"Viterbi Probabilities:\")\n",
    "for t in range(len(sentence)):\n",
    "    print(f\"Position {t + 1}: {viterbi[:, t]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a403b48-aff8-4df5-bdfc-2bd3504c8af7",
   "metadata": {},
   "source": [
    "**Question 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060a3eb-09a2-4beb-85f8-2df879b97d8c",
   "metadata": {},
   "source": [
    "**spaCy Dependency Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e69c6db-2ae6-4220-9dd9-38692279c953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.2/12.8 MB 2.4 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.4/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.6/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.9/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.5/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.8/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 2.9/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.0/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.4/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.5/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.7/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.8/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.4/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.7/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 7.0/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.7/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.6/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/bb1d3772-2681-45e2-b5bf-16a7faf9aea1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231027%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231027T042122Z&X-Amz-Expires=300&X-Amz-Signature=bc80882baddd00b351f53ba8cbf7e15d0ec34ce0087732a2619d6b75770778ff&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.7.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/bb1d3772-2681-45e2-b5bf-16a7faf9aea1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231027%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231027T042122Z&X-Amz-Expires=300&X-Amz-Signature=bc80882baddd00b351f53ba8cbf7e15d0ec34ce0087732a2619d6b75770778ff&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.7.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/bb1d3772-2681-45e2-b5bf-16a7faf9aea1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231027%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231027T042122Z&X-Amz-Expires=300&X-Amz-Signature=bc80882baddd00b351f53ba8cbf7e15d0ec34ce0087732a2619d6b75770778ff&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.7.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/bb1d3772-2681-45e2-b5bf-16a7faf9aea1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231027%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231027T042122Z&X-Amz-Expires=300&X-Amz-Signature=bc80882baddd00b351f53ba8cbf7e15d0ec34ce0087732a2619d6b75770778ff&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.7.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, POS: DET, Dependency: det\n",
      "Token: quick, POS: ADJ, Dependency: amod\n",
      "Token: brown, POS: ADJ, Dependency: amod\n",
      "Token: fox, POS: NOUN, Dependency: nsubj\n",
      "Token: jumps, POS: VERB, Dependency: ROOT\n",
      "Token: over, POS: ADP, Dependency: prep\n",
      "Token: the, POS: DET, Dependency: det\n",
      "Token: lazy, POS: ADJ, Dependency: amod\n",
      "Token: dog, POS: NOUN, Dependency: pobj\n",
      "Token: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract information, such as tokens, POS tags, and dependency relations\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1121ee-beae-4426-91cf-6e556bf262c1",
   "metadata": {},
   "source": [
    "**NLTK Dependency Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd21e502-28d4-4246-9cb8-557dffe5fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, POS: DT\n",
      "Token: quick, POS: JJ\n",
      "Token: brown, POS: NN\n",
      "Token: fox, POS: NN\n",
      "Token: jumps, POS: VBZ\n",
      "Token: over, POS: IN\n",
      "Token: the, POS: DT\n",
      "Token: lazy, POS: JJ\n",
      "Token: dog, POS: NN\n",
      "Token: ., POS: .\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Tokenize and perform POS tagging\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print POS tags\n",
    "for token, pos in pos_tags:\n",
    "    print(f\"Token: {token}, POS: {pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cb1f05-a4ab-49eb-84a0-ff6be4e3110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps [ROOT]\n",
      "  fox [nsubj]\n",
      "    The [det]\n",
      "    quick [amod]\n",
      "    brown [amod]\n",
      "  over [prep]\n",
      "    dog [pobj]\n",
      "      the [det]\n",
      "      lazy [amod]\n",
      "  . [punct]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Define a function to print the parse tree with indentation\n",
    "def print_tree(node, indent=\"\"):\n",
    "    print(indent + f\"{node.text} [{node.dep_}]\")\n",
    "    for child in node.children:\n",
    "        print_tree(child, indent + \"  \")\n",
    "\n",
    "# Find the root of the parse tree (the main verb)\n",
    "for token in doc:\n",
    "    if token.dep_ == \"ROOT\":\n",
    "        root = token\n",
    "\n",
    "# Print the organized parse tree starting from the root\n",
    "print_tree(root)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
